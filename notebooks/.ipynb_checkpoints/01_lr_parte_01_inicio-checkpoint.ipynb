{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regressão linear com **polynomial features** é uma técnica utilizada para modelar relações não-lineares entre variáveis preditoras (features) e a variável de resposta, ampliando a capacidade da regressão linear tradicional. Embora o nome \"regressão linear\" sugira que o modelo lida apenas com relações lineares, isso se refere à linearidade dos **coeficientes** (pesos $ w $) em relação às variáveis, e não à forma como essas variáveis aparecem na equação. Isso significa que a relação entre as variáveis e a saída pode ser não-linear, desde que os coeficientes $ w_0, w_1, \\dots, w_n $ permaneçam lineares no modelo.\n",
    "\n",
    "### Como funciona:\n",
    "Na regressão linear padrão, o modelo ajusta uma função da forma $ y = w_0 + w_1x_1 + \\dots + w_nx_n $, onde $ x_1, x_2, \\dots, x_n $ são as variáveis preditoras e $ w_0, w_1, \\dots, w_n $ são os coeficientes que o modelo aprende. A \"linearidade\" aqui se refere ao fato de que a equação é linear em relação aos coeficientes $ w $, ou seja, cada coeficiente multiplica uma variável ou uma combinação de variáveis diretamente.\n",
    "\n",
    "Quando utilizamos **polynomial features**, introduzimos novas features elevando as variáveis originais a diferentes potências. Por exemplo, para um modelo com uma única variável $ x $ e grau 2, o modelo se torna:\n",
    "$$ y = w_0 + w_1x + w_2x^2 $$\n",
    "Agora o modelo ajusta uma parábola em vez de uma linha reta, mas ainda assim continua sendo linear em relação aos coeficientes $ w_0, w_1, w_2 $.\n",
    "\n",
    "Esse conceito pode ser expandido para múltiplas variáveis e para graus maiores. Para um grau 3 com duas variáveis $ x_1 $ e $ x_2 $, o modelo incluiria termos como $ x_1^2 $, $ x_1x_2 $, $ x_2^3 $, e assim por diante.\n",
    "\n",
    "### PolynomialFeatures no Scikit-Learn:\n",
    "No Scikit-Learn, a transformação de dados para incluir essas novas features polinomiais é feita através do `PolynomialFeatures`, um transformador que gera automaticamente todas as combinações de variáveis até o grau especificado. Esse transformador é geralmente utilizado em conjunto com um regressor linear para criar o modelo final.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures\n",
    "\n",
    "\n",
    "### Benefícios:\n",
    "- **Modelar relações não-lineares**: Adicionando termos polinomiais, o modelo pode capturar padrões mais complexos nos dados.\n",
    "- **Simplicidade**: Apesar de aumentar a complexidade do modelo, o conceito ainda é baseado na regressão linear, o que facilita a interpretação e a implementação.\n",
    "\n",
    "### Desvantagens:\n",
    "- **Overfitting**: Com graus muito altos, o modelo pode se ajustar demais aos dados de treino, capturando ruído em vez de padrões genuínos.\n",
    "- **Crescimento exponencial das features**: À medida que o grau aumenta, o número de termos polinomiais cresce rapidamente, o que pode tornar o modelo mais difícil de treinar e interpretar.\n",
    "\n",
    "Portanto, a regressão linear com **polynomial features** é uma maneira eficaz de lidar com relações não-lineares, mantendo a linearidade em relação aos coeficientes $ w $. Essa técnica oferece uma solução balanceada para problemas onde a relação entre as variáveis preditoras e a variável de resposta é complexa, mas ainda acessível para interpretação e implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, ValidationCurveDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from src.auxiliares import dataframe_coeficientes\n",
    "from src.config import DADOS_EXEMPLO_POLINOMIAL\n",
    "from src.graficos import plot_coeficientes, plot_residuos\n",
    "from src.modelos import grid_search_cv_regressor\n",
    "\n",
    "sns.set_theme(palette=\"bright\")\n",
    "\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DADOS_EXEMPLO_POLINOMIAL)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X = df[[\"X\"]]\n",
    "y = df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "xfit = np.linspace(-0.1, 1.1, 1000)\n",
    "df_xfit = pd.DataFrame({\"X\": xfit})\n",
    "\n",
    "ordens = (1, 3, 8)\n",
    "titulos = (\"Grau 1\", \"Grau 3\", \"Grau 8\")\n",
    "parametros_modelos = {}\n",
    "regressor = LinearRegression()\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 10), tight_layout=True)\n",
    "\n",
    "for ordem, titulo, ax in zip(ordens, titulos, axes.flatten()[:3]):\n",
    "    model = Pipeline([(\"poly\", PolynomialFeatures(ordem, include_bias=False)), (\"reg\", regressor)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(df_xfit)\n",
    "\n",
    "    parametros_modelos[titulo] = {\n",
    "        \"modelo\": model,\n",
    "        \"coef\": model.named_steps[\"reg\"].coef_,\n",
    "        \"intercept\": model.named_steps[\"reg\"].intercept_,\n",
    "    }\n",
    "\n",
    "    ax.scatter(X_train, y_train, s=50, color=\"C0\", label=\"treino\")\n",
    "    ax.scatter(X_test, y_test, s=50, color=\"C2\", label=\"teste\")\n",
    "\n",
    "    ax.plot(xfit, y_pred, color=\"C3\", label=\"predição\")\n",
    "\n",
    "    ax.set_title(titulo)\n",
    "    ax.set_xlim(-0.1, 1.0)\n",
    "    ax.set_ylim(-2, 13)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "for ordem, titulo, ax in zip(ordens, titulos, axes.flatten()[3:6]):\n",
    "    model = Pipeline([(\"poly\", PolynomialFeatures(ordem, include_bias=False)), (\"reg\", regressor)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(df_xfit)\n",
    "\n",
    "    ax.scatter(X_train, y_train, s=50, color=\"C0\", label=\"treino\")\n",
    "\n",
    "    ax.plot(xfit, y_pred, color=\"C3\", label=\"predição\")\n",
    "\n",
    "    ax.set_title(titulo)\n",
    "    ax.set_xlim(-0.1, 1.0)\n",
    "    ax.set_ylim(-2, 13)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "for ordem, titulo, ax in zip(ordens, titulos, axes.flatten()[6:]):\n",
    "    model = Pipeline([(\"poly\", PolynomialFeatures(ordem, include_bias=False)), (\"reg\", regressor)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(df_xfit)\n",
    "\n",
    "    ax.scatter(X_test, y_test, s=50, color=\"C2\", label=\"teste\")\n",
    "\n",
    "    ax.plot(xfit, y_pred, color=\"C3\", label=\"predição\")\n",
    "\n",
    "    ax.set_title(titulo)\n",
    "    ax.set_xlim(-0.1, 1.0)\n",
    "    ax.set_ylim(-2, 13)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "\n",
    "fig.suptitle(f\"Comparando ordens\\n\", fontsize=16)\n",
    "fig.legend(\n",
    "    loc=\"lower center\",\n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(0.5, 0.925),\n",
    "    bbox_transform=fig.transFigure,\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
